{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MonocularDepthEstimation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBciS_TD3JVl"
      },
      "source": [
        "# CS231a PSET 3 Problem 3: Supervised Monocular Depth Estimation\n",
        "In this problem we will train a big deep learning model to do monocular depth estimation.\n",
        "\n",
        "**Using a GPU**. Make sure to first change your runtime to use a GPU: click Runtime -> Change runtime type -> Hardware Accelerator -> GPU and your Colab instance will automatically be backed by GPU compute.\n",
        "\n",
        "First, you should upload the files in the 'p3/code' directory as well as the empty 'checkpoints' directory onto a location of your choosing in Drive. Also, you will need to put the [CLEVR-D](https://drive.google.com/file/d/1IM6gWAZSxae6iVUeEz9BeT73rvr8RwWf/view?usp=sharing) dataset in the same folder, by clicking 'Add shortcut to Drive' and moving it to that folder. You could also simply download the zip file and upload it in that folder, if you have the space needed.\n",
        "\n",
        "Then, run the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwQRvE4C3gf5"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# enter the foldername in your Drive where you have saved the .py files in 'p3/code' needed for this problem\n",
        "# e.g. 'cs231a/monocular_depth_estimation':\n",
        "FOLDERNAME = 'cs231a/monocular_depth_estimation'\n",
        "\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "%ls .\n",
        "%cd drive/MyDrive\n",
        "%cd $FOLDERNAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55cphY32S2uM"
      },
      "source": [
        "If all is set up correctly, you should be able to go to drive/MyDrive/{path to your folder} in the file explorer on the right and see the code files and shortcut to the zip file. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM_rDf34_xC5"
      },
      "source": [
        "# Checking out the data\n",
        "\n",
        "Let's start by having a look at what's in our CLEVR-D dataset. For that, finish the marked sections in data.py, and then run the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJqDYf2__-HF"
      },
      "source": [
        "import data\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from importlib import reload \n",
        "reload(data)\n",
        "plt.rcParams['figure.figsize'] = [8,10]\n",
        "plt.rcParams['figure.dpi'] = 100 \n",
        "\n",
        "train_data_loader, test_data_loader = data.get_data_loaders(\"cs231a-clevr-rgbd.zip\",\n",
        "                                                is_mono=True,\n",
        "                                                batch_size=16,#small batch size to conserve memory\n",
        "                                                train_test_split=0.8,\n",
        "                                                pct_dataset=0.5)#half of dataset to keep things fast\n",
        "test_data_iter = iter(test_data_loader)\n",
        "data_sample = next(test_data_iter)\n",
        "print(\"\\nMean, min and max of RGB image - %.3f %.3f %.3f\"%(\n",
        "                                          torch.mean(data_sample['rgb']),\n",
        "                                          torch.min(data_sample['rgb']),\n",
        "                                          torch.max(data_sample['rgb'])))\n",
        "\n",
        "print(\"Mean, min and max of depth image - %.3f %.3f %.3f\\n\"%(\n",
        "                                          torch.mean(data_sample['depth']),\n",
        "                                          torch.min(data_sample['depth']),\n",
        "                                          torch.max(data_sample['depth'])))\n",
        "\n",
        "rgb_tensor_to_image, depth_tensor_to_image = data.get_tensor_to_image_transforms()\n",
        "fig, axs = plt.subplots(3, 2)\n",
        "axs[0,0].set_title('RGB', size='large')\n",
        "axs[0,1].set_title('Depth', size='large')\n",
        "for i in range(3):\n",
        "    axs[i, 0].imshow(rgb_tensor_to_image(data_sample['rgb'][i]))\n",
        "    axs[i, 1].imshow(depth_tensor_to_image(data_sample['depth'][i]), cmap='gray')\n",
        "    axs[i, 0].axis('off')\n",
        "    axs[i, 1].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTffBCOz7qwz"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "Next, we can go ahead and train the model once you complete the appropriate parts of losses.py and training.py. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2a_1JM754no"
      },
      "source": [
        "Before we run training, let's visualize the training progress using [Tensorboard](https://www.tensorflow.org/tensorboard). When you run the following, you should see the scalars tab showing the loss gradually going down once training starts. If you go to the 'images' tab, you can also be able to observe the 'Ours' images getting better over time, with the 'Diff' images showing less disparity from the ground truth over time. Hit the refresh icon on the top right once you get training going in the next bit, and you should be able to see stuff show up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu8yEUgBPpfM"
      },
      "source": [
        "!pip install tensorboardX\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"/content/drive/MyDrive/$FOLDERNAME/runs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first initialize the model to pass into the training function and confirm that given an rgb image it outputs a depth image."
      ],
      "metadata": {
        "id": "9veZxXzG7Kch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import model\n",
        "from utils import colorize\n",
        "\n",
        "dense_depth_model = model.DenseDepth(encoder_pretrained=False)\n",
        "dense_depth_model = dense_depth_model.to('cuda')\n",
        "sample_image = next(test_data_iter)\n",
        "with torch.no_grad():\n",
        "    model_out = dense_depth_model(sample_image['rgb'].to('cuda')) \n",
        "fig, axs = plt.subplots(1, 2)\n",
        "axs[0].imshow(rgb_tensor_to_image(sample_image['rgb'][0]))\n",
        "axs[1].imshow(depth_tensor_to_image(model_out[0]),cmap='gray')\n",
        "axs[0].axis('off')\n",
        "axs[1].axis('off')\n",
        "del sample_image\n",
        "del model_out       "
      ],
      "metadata": {
        "id": "jDgjjdzM7ONX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In initializing the model we could actually use feature transfer from prior training on [ImageNet](https://en.wikipedia.org/wiki/ImageNet) -- which is a common practice -- by setting encoder_pretrained=True. However, since our data is quite different from that of ImageNet that would actually hinder learning rather than improve it. You can try it if you'd like and see how it impacts the final results!"
      ],
      "metadata": {
        "id": "nWuWbJM0yKEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also make sure the model is correctly loaded onto the GPU and checks its size. Under Memory-Usage, you can see that it takes up a few gigabytes of this GPU's memory, making it a fairly large model:\n",
        "\n"
      ],
      "metadata": {
        "id": "vb3nUVHlAaBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi "
      ],
      "metadata": {
        "id": "bDTnkggDAftK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, once you finish up the TODO parts in losses.py and training.py you can start training!"
      ],
      "metadata": {
        "id": "6_o6xWVE7V3U"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZqgjjPU9HvL"
      },
      "source": [
        "import training \n",
        "import torch \n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "training = reload(training)#reload when debugging to have updated code\n",
        "training.train(5, train_data_loader, test_data_loader, lr=0.0001, model=dense_depth_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yay! If you implemented everything correctly, the loss went down and you saw the model work well. There should be files in the checkpoints directory now, which correspond to model weights throughout different points in training. We won't be using these, but it's standard in deep learning to generate these to later load for running the model for experiments or for futher fine-tuning. We can now again take a look at its output for a given image and see what it does on test set inputs:"
      ],
      "metadata": {
        "id": "KnrNcDV7VJ9R"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv15PJoHy4sw"
      },
      "source": [
        "#we'll iterate to pick a nice set of images\n",
        "for i in range(5): # feel free to change this to see other outputs\n",
        "    sample_image = next(test_data_iter)\n",
        "with torch.no_grad():\n",
        "    model_out = dense_depth_model(sample_image['rgb'].to('cuda')) \n",
        "fig, axs = plt.subplots(3, 3)\n",
        "axs[0,0].set_title('RGB', size='large')\n",
        "axs[0,1].set_title('Predicted Depth', size='large')\n",
        "axs[0,2].set_title('True Depth', size='large')\n",
        "depth_inverse_normalize = data.get_inverse_transforms()[1]\n",
        "for i in range(3):\n",
        "    axs[i, 0].imshow(rgb_tensor_to_image(sample_image['rgb'][i]))\n",
        "    axs[i, 1].imshow(depth_inverse_normalize(model_out[i]).data.cpu().numpy()[0], cmap='gray')\n",
        "    axs[i, 2].imshow(depth_tensor_to_image(sample_image['depth'][i]), cmap='gray')\n",
        "    axs[i, 0].axis('off')\n",
        "    axs[i, 1].axis('off')\n",
        "    axs[i, 2].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the model is largely doing the right thing, although if you look closely it's also evident it is not able to capture some of the smaller nuances of depth disparities between objects."
      ],
      "metadata": {
        "id": "DPxk3nJXPqjw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HImhN6DFTkfi"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "That's it! You have now trained a model for monocular depth estimation. As noted in the PDF, you now just need to download this notebook to submit alongside your python files.\n",
        "\n",
        "Credits: this assignment was adapted from [this](https://github.com/pranjaldatta/DenseDepth-Pytorch) code base.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Yk5uX2dUrl"
      },
      "source": [
        "# (Extra credit) Representation learning an autoencoder\n",
        "\n",
        "As we have done in the problem 2, representation learning can be used to (possibly) speed up and improve learning. An approach we could try for this problem is to use an [autoencoder](https://www.jeremyjordan.me/autoencoders/). Modifying the code to do this is actually not too challenging, since we just need to modify the dataset to output greyscale version of the RGB images and train a model to go from RGB images to greyscale images as before; the DenseNet model that acts as an encoder already outputs a 'bottleneck' representation, and so we'd just want to train it to do the RGB->greyscale conversion and then use the resulting features when starting to the rgb->depth training. The codebase is not currently set up to do this, but as per the PDF you can feel free to tweak it and see what you get!"
      ]
    }
  ]
}